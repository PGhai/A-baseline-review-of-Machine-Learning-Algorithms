{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.io import arff\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB        \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import sklearn.metrics           # For accuracy_score\n",
    "import sklearn.model_selection   # For GridSearchCV and RandomizedSearchCV\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)  # Ignore sklearn deprecation warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)       # Ignore sklearn deprecation warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Reader:\n",
    "        \n",
    "    def readData(self,filePath):\n",
    "        df= pd.read_csv(filePath)\n",
    "        return (df)\n",
    "reader = Reader()     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_of_Results= dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adultData_preprocessing(data,test_data):\n",
    "    \n",
    "    tS = np.array(test_data['Salary'])\n",
    "    tS = [x[:-1] for x in tS]\n",
    "    test_data['Salary'] = tS\n",
    "\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    le.fit(data['Salary'])\n",
    "    data['Salary'] = le.transform(data['Salary'])\n",
    "    test_data['Salary'] = le.transform(test_data['Salary'])\n",
    "\n",
    "    le.fit(data['native-country'])\n",
    "    data['native-country'] = le.transform(data['native-country'])\n",
    "    test_data['native-country'] = le.transform(test_data['native-country'])\n",
    "\n",
    "    le.fit(data[\"marital-status\"])\n",
    "    data[\"marital-status\"] =le.transform(data[\"marital-status\"])\n",
    "    test_data['marital-status'] = le.transform(test_data['marital-status'])\n",
    "\n",
    "    le.fit(data[\"workclass\"])\n",
    "    data[\"workclass\"] =le.transform(data[\"workclass\"])\n",
    "    test_data['workclass'] = le.transform(test_data['workclass'])\n",
    "\n",
    "    le.fit(data[\"relationship\"])\n",
    "    data[\"relationship\"] =le.transform(data[\"relationship\"])\n",
    "    test_data['relationship'] = le.transform(test_data['relationship'])\n",
    "\n",
    "    le.fit(data[\"race\"])\n",
    "    data[\"race\"] =le.transform(data[\"race\"])\n",
    "    test_data['race'] = le.transform(test_data['race'])\n",
    "\n",
    "    le.fit(data[\"sex\"])\n",
    "    data[\"sex\"] =le.transform(data[\"sex\"])\n",
    "    test_data['sex'] = le.transform(test_data['sex'])\n",
    "\n",
    "    le.fit(data[\"occupation\"])\n",
    "    data[\"occupation\"] =le.transform(data[\"occupation\"])\n",
    "    test_data[\"occupation\"] =le.transform(test_data[\"occupation\"])\n",
    "\n",
    "    data = data.drop(['education'],axis=1)\n",
    "    test_data = test_data.drop(['education'],axis=1)\n",
    "    return data,test_data\n",
    "\n",
    "# Steel Plates\n",
    "def steelPlates_preprocessing(steelPlates_data):\n",
    "    columns = steelPlates_data.columns\n",
    "    label_names = ['Pastry', 'Z_Scratch', 'K_Scatch', 'Stains',\n",
    "       'Dirtiness', 'Bumps', 'Other_Faults']\n",
    "    steelPlates_data['label'] = steelPlates_data[label_names].apply(\n",
    "    lambda x: ''.join(x.dropna().astype(str)),\n",
    "    axis=1\n",
    "    )\n",
    "    steelPlates_data = steelPlates_data.drop(label_names,axis=1)\n",
    "    return steelPlates_data\n",
    "\n",
    "def yeast_preprocessing(yeast_data):\n",
    "    yeast_data= yeast_data.drop_duplicates()\n",
    "    yeast_data = yeast_data.drop(['Sequence Name'],axis=1)\n",
    "    return yeast_data\n",
    "def ThoraricSurgery_preprocessing(ThoraricSurgery_data):\n",
    "    cols = ThoraricSurgery_data.columns[:-1]\n",
    "    for col in cols:\n",
    "        if (str(ThoraricSurgery_data[col].dtype) == \"object\") and ((len(ThoraricSurgery_data[col].unique()))==2):\n",
    "            dummies = pd.get_dummies(ThoraricSurgery_data[col])\n",
    "            d_col =(dummies.columns)\n",
    "            dummies =dummies.rename(columns={d_col[0]: (col+ str(d_col[0])),\n",
    "                           d_col[1]: (col+str(d_col[1]))},errors=\"raise\")\n",
    "            ThoraricSurgery_data = pd.concat([ThoraricSurgery_data,dummies], axis=1)\n",
    "            ThoraricSurgery_data= ThoraricSurgery_data.drop(col,axis=1)\n",
    "            \n",
    "    le = preprocessing.LabelEncoder()\n",
    "    le.fit(ThoraricSurgery_data['Risk1Yr'])\n",
    "    ThoraricSurgery_data['Risk1Yr']=le.transform(ThoraricSurgery_data['Risk1Yr'])\n",
    "    \n",
    "    le.fit(ThoraricSurgery_data['DGN'])\n",
    "    ThoraricSurgery_data['DGN']=le.transform(ThoraricSurgery_data['DGN'])\n",
    "    \n",
    "    le.fit(ThoraricSurgery_data['PRE6'])\n",
    "    ThoraricSurgery_data['PRE6']=le.transform(ThoraricSurgery_data['PRE6'])\n",
    "    \n",
    "    le.fit(ThoraricSurgery_data['PRE14'])\n",
    "    ThoraricSurgery_data['PRE14'] =le.transform(ThoraricSurgery_data['PRE14'])\n",
    "    \n",
    "    \n",
    "    return ThoraricSurgery_data\n",
    "def seismic_preprocessing(seismic_data):\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    for col in  seismic_data.columns:\n",
    "        if (str(seismic_data[col].dtype) == \"object\"):\n",
    "            le.fit(seismic_data[col])\n",
    "            seismic_data[col] = le.transform(seismic_data[col])\n",
    "            \n",
    "    return seismic_data\n",
    "\n",
    "def breastcancer_preprocessing(data):\n",
    "    data = data.drop(\"ID\", axis=1)\n",
    "    data['Diagnosis'].replace(0, 'M',inplace=True)\n",
    "    data['Diagnosis'].replace(1, 'B',inplace=True)\n",
    "    return data\n",
    "def GermanCredit_preprocessing(data):\n",
    "    new_header = data.iloc[0] #grab the first row for the header\n",
    "    data = data[1:] #take the data less the header row\n",
    "    data.columns = new_header #set the header row as the df header\n",
    "#     data = data.drop(\"ID\", axis=1)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "# from sklearn.svm import SVC\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.ensemble import AdaBoostClassifier\n",
    "# from sklearn.naive_bayes import GaussianNB        \n",
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "# from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "def Models(X_train,y_train,X_test,y_test):\n",
    "    \n",
    "    Results ={}\n",
    "#     clf = LogisticRegression(random_state=0, solver='lbfgs', multi_class='multinomial').fit(X_train, y_train)\n",
    "    \n",
    "#     Results.update({\"LR\":clf.score(X_test,y_test)})\n",
    "    \n",
    "#     K- nearest Neighbors\n",
    "\n",
    "    neigh = KNeighborsClassifier()\n",
    "    \n",
    "#     Hyperparameter tuning KNN\n",
    "\n",
    "    n_neighbors_grid=np.array([7,9,11,13])\n",
    "    param_grid = { 'n_neighbors' : n_neighbors_grid}\n",
    "    gridcv = GridSearchCV(neigh, param_grid, verbose=1, cv=3)\n",
    "    \n",
    "    gridcv.fit(X_train,y_train) \n",
    "    Results.update({\"KNN\":gridcv.best_estimator_.score(X_test,y_test)})\n",
    "\n",
    "#     rfc = RandomForestClassifier(n_estimators=100, max_depth=6, random_state=0)\n",
    "#     rfc.fit(X_train,y_train)\n",
    "#     Results.update({\"RandomForest\":rfc.score(X_test,y_test)})\n",
    "    \n",
    "#     abc = AdaBoostClassifier(n_estimators=100, random_state=0)\n",
    "#     abc.fit(X_train, y_train) \n",
    "#     Results.update({\"AdaBoostClassifier\":abc.score(X_test,y_test)})\n",
    "    \n",
    "#     gnb = GaussianNB()\n",
    "#     y_pred = gnb.fit(X_train, y_train).predict(X_test)\n",
    "#     actual_y = np.array(y_test)\n",
    "#     score = ( 1-(((actual_y != y_pred).sum())/len(y_pred)))\n",
    "#     Results.update({\"GNB\":score})\n",
    "    \n",
    "#     tree = DecisionTreeClassifier(random_state=0)\n",
    "#     tree.fit(X_train,y_train)\n",
    "#     Results.update({\"DecisionTree\":tree.score(X_test,y_test)})\n",
    "    \n",
    "#     mlp = MLPClassifier()\n",
    "#     mlp.fit(X_train,y_train)\n",
    "#     Results.update({\"MLP\":mlp.score(X_test,y_test)})\n",
    "    \n",
    "    return Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:91: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "\n",
    "Adult_data= reader.readData(r\"C:\\Users\\admin\\Desktop\\Fall19\\ML\\data\\adult\\adult_data.csv\")\n",
    "Adult_Test = reader.readData(r\"C:\\Users\\admin\\Desktop\\Fall19\\ML\\data\\adult\\adult_test_set.csv\")\n",
    "\n",
    "steelPlates_data= reader.readData(r\"C:\\Users\\admin\\Desktop\\Fall19\\ML\\data\\SteelPlates\\SteelPlates.csv\")\n",
    "yeast_data= reader.readData(r\"C:\\Users\\admin\\Desktop\\Fall19\\ML\\data\\yeast\\yeast.csv\")\n",
    "ThoraricSurgery_data = arff.loadarff(r'C:\\Users\\admin\\Desktop\\Fall19\\ML\\data\\thropatic\\ThoraricSurgery.arff')\n",
    "seismic_data= reader.readData(r\"C:\\Users\\admin\\Desktop\\Fall19\\ML\\data\\sesmic\\seismic-bumps.csv\")\n",
    "\n",
    "messidor_features_data=arff.loadarff(r'C:\\Users\\admin\\Desktop\\Fall19\\ML\\data\\Diabetic\\messidor_features.arff')\n",
    "creditcard_data=reader.readData(r'C:\\Users\\admin\\Desktop\\Fall19\\ML\\data\\CreditCard\\creditcard_default.csv')\n",
    "breastcancer_data=reader.readData(r'C:\\Users\\admin\\Desktop\\Fall19\\ML\\data\\breastCancer\\breastcancer.csv')\n",
    "ausCrediApproval_data=reader.readData(r'C:\\Users\\admin\\Desktop\\Fall19\\ML\\data\\StatlogAus\\ausCreditApproval.csv')\n",
    "GermanCredit_data=reader.readData(r'C:\\Users\\admin\\Desktop\\Fall19\\ML\\data\\StatlogGerman\\GermanCredit.csv')\n",
    "\n",
    "DataSets = { \"steelPlates_data\":steelPlates_data,\n",
    "             \"yeast_data\":yeast_data,\n",
    "            \"ThoraricSurgery_data\":ThoraricSurgery_data,\n",
    "            \"seismic_data\":seismic_data,\n",
    "           \"messidor_features_data\":messidor_features_data,\n",
    "           \"creditcard_data\":creditcard_data,\n",
    "           \"breastcancer_data\":breastcancer_data,\n",
    "           \"ausCrediApproval_data\":ausCrediApproval_data,\n",
    "           \"GermanCredit_data\":GermanCredit_data}\n",
    "\n",
    "Adult_data, Adult_Test = adultData_preprocessing(Adult_data, Adult_Test)\n",
    "steelPlates_data = steelPlates_preprocessing(steelPlates_data)\n",
    "yeast_data= yeast_preprocessing(yeast_data)\n",
    "\n",
    "ThoraricSurgery_data = pd.DataFrame(ThoraricSurgery_data[0])\n",
    "ThoraricSurgery_data = ThoraricSurgery_preprocessing(ThoraricSurgery_data)\n",
    "\n",
    "seismic_data=seismic_data.drop_duplicates()\n",
    "seismic_data = seismic_preprocessing(seismic_data)\n",
    "\n",
    "GermanCredit_data= GermanCredit_preprocessing(GermanCredit_data)\n",
    "\n",
    "breastcancer_data= breastcancer_preprocessing(breastcancer_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def createTestTrain(data,name):\n",
    "    if name == \"steelPlates_data\":\n",
    "        X = np.array(data.drop(['label'],axis=1))\n",
    "        y = np.array(data['label'])\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "        return X_train, X_test, y_train, y_test\n",
    "    if name == \"yeast_data\":\n",
    "        X = np.array(data.drop(['Label'],axis=1))\n",
    "        y = np.array(data['Label'])\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "        return X_train, X_test, y_train, y_test\n",
    "    if name == \"ThoraricSurgery_data\":\n",
    "        X = np.array(data.drop(['Risk1Yr'],axis=1))\n",
    "        y = np.array(data['Risk1Yr'])\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "        return X_train, X_test, y_train, y_test\n",
    "    if name ==\"seismic_data\":\n",
    "        X = np.array(data.drop(['class'],axis=1))\n",
    "        y = np.array(data['class'])\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "        return X_train, X_test, y_train, y_test\n",
    "    if name =\"messidor_features_data\":\n",
    "        X = np.array(data.drop(['19'],axis=1))\n",
    "        y = np.array(data['19'])\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "        return  X_train, X_test, y_train, y_test\n",
    "    if name =\"creditcard_data\":\n",
    "        X = np.array(data.drop(['default payment next month'],axis=1))\n",
    "        y = np.array(data['default payment next month'])\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "    if name =\"breastcancer_data\":\n",
    "        X = np.array(data.drop(['Diagnosis'],axis=1))\n",
    "        y = np.array(data['Diagnosis'])\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42) \n",
    "    if name =\"breastcancer_data\":\n",
    "        X = np.array(data.drop(['default payment next month'],axis=1))\n",
    "        y = np.array(data['default payment next month'])\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "    if name =\"ausCrediApproval_data\":\n",
    "        X = np.array(data.drop(['default payment next month'],axis=1))\n",
    "        y = np.array(data['default payment next month'])\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "    else:\n",
    "        y_train = Adult_data['Salary']\n",
    "        y_test = Adult_Test['Salary']\n",
    "        X_train = Adult_data.drop(['Salary'],axis=1)\n",
    "        X_test = Adult_Test.drop(['Salary'],axis=1)\n",
    "        return X_train, X_test, y_train, y_test\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for steelPlates_data\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  12 out of  12 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for yeast_data\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  12 out of  12 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  12 out of  12 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for ThoraricSurgery_data\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "Results for seismic_data\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  12 out of  12 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for messidor_features_data\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  12 out of  12 | elapsed:    9.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for creditcard_data\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  12 out of  12 | elapsed:    8.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for breastcancer_data\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  12 out of  12 | elapsed:    8.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for ausCrediApproval_data\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  12 out of  12 | elapsed:    8.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for GermanCredit_data\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  12 out of  12 | elapsed:    9.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  12 out of  12 | elapsed:    9.6s finished\n"
     ]
    }
   ],
   "source": [
    "def getResults(DataSets,dict_of_Results):\n",
    "\n",
    "    for data in DataSets.keys():\n",
    "        print(\"Results for\",data,end='')\n",
    "        print('')\n",
    "        X_train, X_test, y_train, y_test =createTestTrain(DataSets.get(data),data)\n",
    "        dict_of_Results.update({data:Models(X_train,y_train,X_test,y_test)})\n",
    "    X_train, X_test, y_train, y_test =createTestTrain(DataSets.get(data),data)   \n",
    "    dict_of_Results.update({data:Models(X_train,y_train,X_test,y_test)})\n",
    "    return dict_of_Results\n",
    "# Models(X_train,y_train,X_test,y_test)\n",
    "dict_of_Results=getResults(DataSets,dict_of_Results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>steelPlates_data</th>\n",
       "      <th>yeast_data</th>\n",
       "      <th>ThoraricSurgery_data</th>\n",
       "      <th>seismic_data</th>\n",
       "      <th>messidor_features_data</th>\n",
       "      <th>creditcard_data</th>\n",
       "      <th>breastcancer_data</th>\n",
       "      <th>ausCrediApproval_data</th>\n",
       "      <th>GermanCredit_data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>KNN</td>\n",
       "      <td>0.5117</td>\n",
       "      <td>0.567288</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.918919</td>\n",
       "      <td>0.797248</td>\n",
       "      <td>0.797248</td>\n",
       "      <td>0.797248</td>\n",
       "      <td>0.797248</td>\n",
       "      <td>0.797248</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     steelPlates_data  yeast_data  ThoraricSurgery_data  seismic_data  \\\n",
       "KNN            0.5117    0.567288              0.833333      0.918919   \n",
       "\n",
       "     messidor_features_data  creditcard_data  breastcancer_data  \\\n",
       "KNN                0.797248         0.797248           0.797248   \n",
       "\n",
       "     ausCrediApproval_data  GermanCredit_data  \n",
       "KNN               0.797248           0.797248  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(dict_of_Results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \tX_train,X_test=normalize_features(X_train,X_test)\n",
    "\n",
    "# \treturn X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "\n",
    "# def get_accuracy(y_pred,X_test,y_test):\n",
    "#     print(\"Number of mislabeled points out of a total %d points : %d\" % (X_test.shape[0],(y_test.reshape(-1) != y_pred).sum()))\n",
    "#     print(\"Accuracy=%.2f\"%(100-((y_test.reshape(-1) != y_pred).sum()/X_test.shape[0])*100)+'%')\n",
    "\n",
    "\n",
    "# def fit_predict_classfier(classifer,feeding_input):\n",
    "# \tX_train=feeding_input[0]\n",
    "# \tY_train=feeding_input[1]\n",
    "# \tX_test=feeding_input[2]\n",
    "# \treturn(classifer.fit(X_train,Y_train).predict(X_test))\n",
    "# # In[39]:\n",
    "\n",
    "# def call_knn(feeding_input):\n",
    "# \tneigh = fit_predict_classfier(KNeighborsClassifier(n_neighbors=3),feeding_input)\n",
    "# \tget_accuracy(neigh,feeding_input[2],feeding_input[3])\n",
    "\n",
    "# \tn_neighbors_grid=np.array([1,3,5,7,9,11,13,15])\n",
    "# \tknn = KNeighborsClassifier()\n",
    "# \tparam_grid = { 'n_neighbors' : n_neighbors_grid}\n",
    "# \tgridcv = sklearn.model_selection.GridSearchCV(knn, param_grid, verbose=1, cv=3)\n",
    "# \tgridcv.fit(feeding_input[0], feeding_input[1])\n",
    "# \tprint(\"best parameters:\", gridcv.best_params_)\n",
    "# \tprint(\"%.1f%% accuracy on validation sets (average)\" % (gridcv.best_score_*100))\n",
    "\n",
    "# \tneigh_test = fit_predict_classfier(KNeighborsClassifier(n_neighbors=gridcv.best_params_['n_neighbors']),feeding_input)\n",
    "# \tget_accuracy(neigh_test,feeding_input[2],feeding_input[3])\n",
    "\n",
    "# \tprint(gridcv.best_params_['n_neighbors'])\n",
    "# \texit()\n",
    "\n",
    "# def call_svm(feeding_input):\n",
    "# \tsvm_classifier = fit_predict_classfier(SVC(gamma='auto'),feeding_input)\n",
    "# \tget_accuracy(svm_classifier,feeding_input[2],feeding_input[3])\n",
    "\n",
    "# \tC_grid = np.logspace(0, 3, 4)\n",
    "# \tgamma_grid = np.logspace(-2, 1, 4)\n",
    "# \tsvm = SVC(kernel='rbf')\n",
    "# \tparam_grid = { 'C' : C_grid, 'gamma' : gamma_grid}\n",
    "# \tgridcv = sklearn.model_selection.GridSearchCV(svm, param_grid, verbose=1, cv=3)\n",
    "# \tgridcv.fit(feeding_input[0], feeding_input[1])\n",
    "# \tprint(\"best parameters:\", gridcv.best_params_)\n",
    "# \tprint(\"%.1f%% accuracy on validation sets (average)\" % (gridcv.best_score_*100))\n",
    "\n",
    "# def call_DT(feeding_input):\n",
    "# \tdt = fit_predict_classfier(DecisionTreeClassifier(random_state=0),feeding_input)\n",
    "# \tget_accuracy(dt,feeding_input[2],feeding_input[3])\n",
    "\n",
    "# \tcret=np.array(['gini','entropy'])\n",
    "# \tmax_depth=np.array([1,2,3,4,5,6,7,8,9,10])\n",
    "# \tdt = DecisionTreeClassifier(random_state=0)\n",
    "# \tparam_grid ={ 'criterion' : cret, 'max_depth' : max_depth}\n",
    "# \tgridcv = sklearn.model_selection.GridSearchCV(dt, param_grid, verbose=1, cv=3)\n",
    "# \tgridcv.fit(feeding_input[0], feeding_input[1])\n",
    "# \tprint(\"best parameters:\", gridcv.best_params_)\n",
    "# \tprint(\"%.1f%% accuracy on validation sets (average)\" % (gridcv.best_score_*100))\n",
    "\n",
    "\n",
    "# def call_Randomforest(feeding_input):\n",
    "# \tRandomforest = fit_predict_classfier(RandomForestClassifier(max_depth=2, random_state=0),feeding_input)\n",
    "# \tget_accuracy(Randomforest,feeding_input[2],feeding_input[3])\n",
    "\n",
    "# \tn_estimators=np.array([100,200,300,400,500])\n",
    "# \tmax_depth=np.array([1,2,3,4,5,6,7,8,9,10])\n",
    "# \trf = RandomForestClassifier(random_state=0)\n",
    "# \tparam_grid ={ 'n_estimators' : n_estimators, 'max_depth' : max_depth}\n",
    "# \tgridcv = sklearn.model_selection.GridSearchCV(rf, param_grid, verbose=1, cv=3)\n",
    "# \tgridcv.fit(feeding_input[0], feeding_input[1])\n",
    "# \tprint(\"best parameters:\", gridcv.best_params_)\n",
    "# \tprint(\"%.1f%% accuracy on validation sets (average)\" % (gridcv.best_score_*100))\n",
    "\n",
    "# def call_adaboost(feeding_input):\n",
    "# \tadaboost = fit_predict_classfier(AdaBoostClassifier(n_estimators=100, random_state=0),feeding_input)\n",
    "# \tget_accuracy(adaboost,feeding_input[2],feeding_input[3])\n",
    "\n",
    "# \tn_estimators=np.array([100,200,300,400,500])\n",
    "# \tada = AdaBoostClassifier(random_state=0)\n",
    "# \tparam_grid ={ 'n_estimators' : n_estimators}\n",
    "# \tgridcv = sklearn.model_selection.GridSearchCV(ada, param_grid, verbose=1, cv=3)\n",
    "# \tgridcv.fit(feeding_input[0], feeding_input[1])\n",
    "# \tprint(\"best parameters:\", gridcv.best_params_)\n",
    "# \tprint(\"%.1f%% accuracy on validation sets (average)\" % (gridcv.best_score_*100))\n",
    "\n",
    "# def call_logisticRegression(feeding_input):\n",
    "# \tlr = fit_predict_classfier(LogisticRegression(random_state=0),feeding_input)\n",
    "# \tget_accuracy(lr,feeding_input[2],feeding_input[3])\n",
    "\n",
    "# \tmax_iter=np.array([100,200,300,400,500])\n",
    "# \tlr = LogisticRegression(random_state=0)\n",
    "# \tparam_grid ={ 'max_iter' : max_iter}\n",
    "# \tgridcv = sklearn.model_selection.GridSearchCV(lr, param_grid, verbose=1, cv=3)\n",
    "# \tgridcv.fit(feeding_input[0], feeding_input[1])\n",
    "# \tprint(\"best parameters:\", gridcv.best_params_)\n",
    "# \tprint(\"%.1f%% accuracy on validation sets (average)\" % (gridcv.best_score_*100))\n",
    "\n",
    "# def call_NaiveBayes(feeding_input):\n",
    "# \tNB_y_pred = fit_predict_classfier(GaussianNB(),feeding_input)\n",
    "# \tget_accuracy(NB_y_pred,feeding_input[2],feeding_input[3])\n",
    "\n",
    "# \t# max_iter=np.array([100,200,300,400,500])\n",
    "# \t# lr = LogisticRegression(random_state=0)\n",
    "# \t# param_grid ={ 'max_iter' : max_iter}\n",
    "# \t# gridcv = sklearn.model_selection.GridSearchCV(lr, param_grid, verbose=1, cv=3)\n",
    "# \t# gridcv.fit(feeding_input[0], feeding_input[1])\n",
    "# \t# print(\"best parameters:\", gridcv.best_params_)\n",
    "# \t# print(\"%.1f%% accuracy on validation sets (average)\" % (gridcv.best_score_*100))\n",
    "\n",
    "# def call_NN(feeding_input):\n",
    "# \tNN = fit_predict_classfier(MLPClassifier(solver='lbfgs', alpha=1e-5,hidden_layer_sizes=(5, 2), random_state=1),feeding_input)\n",
    "# \tget_accuracy(NN,feeding_input[2],feeding_input[3])\n",
    "\n",
    "# def call_all_classifiers(feeding_input):\n",
    "# \t#knn\n",
    "# \tprint('\\nknn')\n",
    "# \tcall_knn(feeding_input)\n",
    "\n",
    "# \t#svm\n",
    "# \tprint('\\nsvm')\n",
    "# \tcall_svm(feeding_input)\n",
    "\n",
    "\n",
    "\n",
    "# \t#decisiontree\n",
    "# \tprint('\\nDT')\n",
    "# \tcall_DT(feeding_input)\n",
    "# \t#Randomforest uncommmmmmmmmmmmmmmmmeeeeeeeeeeeeeeeeennnnnnnnnnnnnnnnnnnnnnnttttttttttttttttt\n",
    "# \tprint('\\nRandomforest')\n",
    "# \t#call_Randomforest(feeding_input)\n",
    "\n",
    "\n",
    "# \t#adaboost\n",
    "# \tprint('\\nadaboost')\n",
    "# \tcall_adaboost(feeding_input)\n",
    "\n",
    "# \t#LogisticRegression\n",
    "# \tprint('\\nLogisticRegression')\n",
    "# \tcall_logisticRegression(feeding_input)\n",
    "\n",
    "# \t#naiveBayes\n",
    "# \tprint('\\nnaiveBayes')\n",
    "# \tcall_NaiveBayes(feeding_input)\n",
    "\n",
    "# \t#NeuralNetwork\n",
    "# \tprint('\\nNeuralNetwork')\n",
    "# \tcall_NN(feeding_input)\n",
    "\n",
    "\n",
    "# # X_train_Diabetic, X_test_Diabetic, y_train_Diabetic, y_test_Diabetic=load_dataset('messidor_features.csv')\n",
    "# # X_train_cc_default, X_test_cc_default, y_train_cc_default, y_test_cc_default=load_dataset('creditcard_default.csv')\n",
    "# # X_train_breastcancer, X_test_breastcancer, y_train_breastcancer, y_test_breastcancer=load_dataset('breastcancer.csv')\n",
    "# # X_train_ausCredit, X_test_ausCredit, y_train_ausCredit, y_test_ausCredit=load_dataset('ausCreditApproval.csv')\n",
    "# # X_train_GermanCredit, X_test_GermanCredit, y_train_GermanCredit, y_test_GermanCredit=load_dataset('GermanCredit.csv')\n",
    "\n",
    "\n",
    "# feeding_input_Diabetic=[]\n",
    "# feeding_input_Diabetic.append(X_train_Diabetic)\n",
    "# feeding_input_Diabetic.append(y_train_Diabetic)\n",
    "# feeding_input_Diabetic.append(X_test_Diabetic)\n",
    "# feeding_input_Diabetic.append(y_test_Diabetic)\n",
    "\n",
    "# feeding_input_cc_default=[]\n",
    "# feeding_input_cc_default.append(X_train_cc_default)\n",
    "# feeding_input_cc_default.append(y_train_cc_default)\n",
    "# feeding_input_cc_default.append(X_test_cc_default)\n",
    "# feeding_input_cc_default.append(y_test_cc_default)\n",
    "\n",
    "# feeding_input_breastcancer=[]\n",
    "# feeding_input_breastcancer.append(X_train_breastcancer)\n",
    "# feeding_input_breastcancer.append(y_train_breastcancer)\n",
    "# feeding_input_breastcancer.append(X_test_breastcancer)\n",
    "# feeding_input_breastcancer.append(y_test_breastcancer)\n",
    "\n",
    "# feeding_input_ausCredit=[]\n",
    "# feeding_input_ausCredit.append(X_train_ausCredit)\n",
    "# feeding_input_ausCredit.append(y_train_ausCredit)\n",
    "# feeding_input_ausCredit.append(X_test_ausCredit)\n",
    "# feeding_input_ausCredit.append(y_test_ausCredit)\n",
    "\n",
    "# feeding_input_GermanCredit=[]\n",
    "# feeding_input_GermanCredit.append(X_train_GermanCredit)\n",
    "# feeding_input_GermanCredit.append(y_train_GermanCredit)\n",
    "# feeding_input_GermanCredit.append(X_test_GermanCredit)\n",
    "# feeding_input_GermanCredit.append(y_test_GermanCredit)\n",
    "\n",
    "\n",
    "\n",
    "# #feeding_input=feeding_input_Diabetic\n",
    "# call_all_classifiers(feeding_input_GermanCredit)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
