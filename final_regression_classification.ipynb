{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Classification\n",
    "<br>Classification Models<br><br>\n",
    "1. KNN classification<br>\n",
    "2. Support vector classification<br>\n",
    "3. Decision tree classification<br>\n",
    "4. Random forest classification<br>\n",
    "5. AdaBoost classification<br>\n",
    "6. Logistic regression for classification<br>\n",
    "7. Gaussian naive bayes classificaiton<br>\n",
    "8. Neural network classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Regression\n",
    "<br>Regression Models<br><br>\n",
    "1. Linear regression<br>\n",
    "2. Linear regression - Ridge<br>\n",
    "3. Support vector regression<br>\n",
    "4. Decision tree regression<br>\n",
    "5. Random forest regression<br>\n",
    "6. AdaBoost regression<br>\n",
    "7. Gaussian process regression<br>\n",
    "8. Neural network regression - MLP Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Linear Regression\n",
    "from sklearn.linear_model import Ridge, SGDRegressor, LinearRegression\n",
    "\n",
    "#Support Vector Regression\n",
    "from sklearn.svm import SVR, NuSVR\n",
    "\n",
    "#Decission Tree Regressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "#Random Forest Regression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "#AdaBoost Regression \n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "\n",
    "#Gaussian Process Regression\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import DotProduct, WhiteKernel, RationalQuadratic, RBF\n",
    "\n",
    "#Neural Network Regression (MLP)\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "#Metrics & Hyperparameter Search\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, mean_squared_log_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "#Classification Libraries\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB        \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "#Standard Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import seaborn as sns\n",
    "import scipy\n",
    "import scipy.stats\n",
    "import time\n",
    "from scipy.io import arff\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)  # Ignore sklearn deprecation warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)       # Ignore sklearn deprecation warnings\n",
    "def warn(*args, **kwargs):\n",
    "    pass \n",
    "warnings.warn = warn\n",
    "np.set_printoptions(precision=3, suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_wine_data_set():\n",
    "    inputDataSet = pd.read_csv('R1_winequality-white.csv')\n",
    "    features = inputDataSet[['fixed_acidity', 'volatile_acidity', 'citric_acid', 'residual_sugar', 'chlorides', 'free_sulfur_dioxide', 'total_sulfur_dioxide', 'density', 'pH', 'sulphates', 'alcohol']]\n",
    "    target = inputDataSet[['quality']]\n",
    "    return features, target\n",
    "\n",
    "def load_communities_data_set():\n",
    "    inputDataSet = pd.read_csv('Imputed_R2_communities.csv')\n",
    "    features = inputDataSet[[ 'Column50', 'Column56', 'Column49', 'Column9', 'Column51',\n",
    "           'Column52', 'Column8', 'Column21', 'Column23', 'Column46', 'Column47',\n",
    "           'Column73', 'Column44', 'Column34', 'Column38', 'Column83', 'Column36',\n",
    "           'Column80', 'Column75', 'Column55', 'Column79', 'Column74', 'Column33',\n",
    "           'Column25', 'Column18', 'Column77', 'Column35', 'Column68', 'Column95',\n",
    "           'Column6', 'Column84', 'Column16', 'Column76', 'Column26', 'Column126',\n",
    "           'Column96', 'Column43', 'Column39', 'Column92', 'Column78', 'Column37',\n",
    "           'Column19', 'Column45', 'Column67', 'Column42', 'Column69', 'Column57',\n",
    "           'Column11', 'Column61']]\n",
    "    target = inputDataSet[['Column128']]\n",
    "    return features, target\n",
    "\n",
    "def load_qasr_data_set():\n",
    "    inputDataSet = pd.read_csv('Processed_R3_qsar_aquatic_toxicity_.csv')\n",
    "    features = inputDataSet[['Column1','Column2','Column3','Column4','Column5','Column6','Column7','Column8']]\n",
    "    target = inputDataSet[['Column9']]\n",
    "    return features, target\n",
    "    \n",
    "def load_parkinson_data_set():\n",
    "    inputDataSet = pd.read_csv('Processed_R4_train_data.csv')\n",
    "    features = inputDataSet[['Column2','Column3','Column4','Column5','Column6','Column7','Column8',\n",
    "                         'Column9', 'Column10', 'Column11', 'Column12', 'Column13', 'Column14', 'Column15',\n",
    "                         'Column16', 'Column17', 'Column18', 'Column19', 'Column20', 'Column21', 'Column22',\n",
    "                         'Column23', 'Column24','Column25','Column26','Column27' ]]\n",
    "    target = inputDataSet[['Column28']]\n",
    "    return features, target\n",
    "\n",
    "def load_bike_data_set():\n",
    "    hourDataSet = pd.read_csv('hour.csv')\n",
    "    features = hourDataSet[['season', 'holiday', 'mnth', 'hr', 'weekday', 'workingday', 'weathersit', 'temp', 'atemp', 'hum', 'windspeed']]\n",
    "    target = hourDataSet[['cnt']]\n",
    "    return features, target\n",
    "\n",
    "def load_concrete_data_set():\n",
    "    filepath = 'Concrete_Data.xls'\n",
    "    concrete = pd.read_excel(filepath)\n",
    "    concrete.columns = concrete.columns.str.strip().str.replace(r\"\\(.*\\)\",'').str.replace(' ', '').str.replace('(', '').str.replace(')', '')\n",
    "    features = concrete[['Cement', 'BlastFurnaceSlag', 'FlyAsh', 'Water', 'Superplasticizer', 'CoarseAggregate', 'FineAggregate', 'Age']]\n",
    "    target = concrete[['Concretecompressivestrength']]\n",
    "    return features, target\n",
    "\n",
    "def load_facebook_data_set():\n",
    "    inputDataSet = pd.read_csv('R5_processed_V2.csv')\n",
    "    features = inputDataSet[[\n",
    "        'Page_total_likes','Type_d3','Type_d2','Type_d1','Category_d3','Category_d2','Category_d1',\n",
    "        'Post_Month_d4','Post_Month_d3','Post_Month_d2','Post_Month_d1',\n",
    "        'Post_Weekday_d1','Post_Weekday_d2','Post_Weekday_d3',\n",
    "        'Post_Hour_d5','Post_Hour_d4','Post_Hour_d3','Post_Hour_d2','Post_Hour_d1',\n",
    "        'Paid','Lifetime_Post_Total_Reach','Lifetime_Post_Total_Impressions',\n",
    "        'Lifetime_Engaged_Users','Lifetime_Post_Consumers','Lifetime_Post_Consumptions',\n",
    "        'Lifetime_Post_Impressions_by_people_who_have_liked_your_Page',\n",
    "        'Lifetime_Post_reach_by_people_who_like_your_Page',\n",
    "        'Lifetime_People_who_have_liked_your_Page_and_engaged_with_your_post',\n",
    "        'comment','like','share' ]]\n",
    "    target = inputDataSet[['Total_Interactions']]\n",
    "    return features, target\n",
    "\n",
    "def load_student_data_set():\n",
    "    studentPorDataSet = pd.read_csv('student-por.csv', sep=';')\n",
    "\n",
    "    #Dropping school, G1 and G2 from dataframe\n",
    "    studentPorDataSet = studentPorDataSet.drop(['school', 'G1', 'G2'], axis=1)\n",
    "\n",
    "    studentPorDataSet = pd.get_dummies(studentPorDataSet)\n",
    "    features = studentPorDataSet.corr().abs()['G3'].sort_values(ascending=False)\n",
    "    features = features[:14]\n",
    "\n",
    "    studentPorDataSet = studentPorDataSet.ix[:, features.index]\n",
    "    studentPorDataSet = studentPorDataSet.drop(columns = 'higher_no')\n",
    "\n",
    "    features = studentPorDataSet[['failures', 'higher_yes', 'studytime', 'Medu', 'Fedu', 'Dalc',\n",
    "           'Walc', 'reason_reputation', 'address_U', 'address_R', 'internet_yes',\n",
    "           'internet_no']]\n",
    "    target = studentPorDataSet[['G3']]\n",
    "    return features, target\n",
    "\n",
    "def load_sgemm_data_set():\n",
    "    sgemmDataSet = pd.read_csv('sgemm_product.csv')\n",
    "    sgemmDataSet.columns = sgemmDataSet.columns.str.strip().str.replace(' ', '_').str.replace('(', '').str.replace(')', '')\n",
    "\n",
    "    features = sgemmDataSet[['MWG','NWG','KWG','MDIMC','NDIMC','MDIMA','NDIMB','KWI','VWM','VWN','STRM','STRN','SA','SB']]\n",
    "    target = sgemmDataSet[['Run1_ms']]\n",
    "    return features, target\n",
    "\n",
    "def load_merck_data_set():\n",
    "    df1 = pd.read_csv('ACT2_competition_training.csv')\n",
    "    df2 = pd.read_csv('ACT4_competition_training.csv')\n",
    "    data = pd.concat([df1, df2])\n",
    "    data = data.fillna(data.mean())\n",
    "    features = data[['D_388', 'D_3806', 'D_1402', 'D_986', 'D_2654', 'D_8446',\n",
    "       'D_2653', 'D_1659', 'D_2655', 'D_1286', 'D_7524', 'D_8303', 'D_1743',\n",
    "       'D_1740', 'D_1489', 'D_1104', 'D_1745', 'D_2472', 'D_982', 'D_7958',\n",
    "       'D_387', 'D_8051', 'D_1658', 'D_4994', 'D_1776', 'D_4545', 'D_1401',\n",
    "       'D_987', 'D_5756', 'D_2652', 'D_1737', 'D_8126', 'D_1577', 'D_2471',\n",
    "       'D_2656', 'D_984', 'D_1744', 'D_3845', 'D_385', 'D_2651', 'D_7117',\n",
    "       'D_1578', 'D_466', 'D_1217', 'D_1218', 'D_1662', 'D_1110', 'D_1109', 'D_8050']]\n",
    "    target = data[['Act']]\n",
    "    return features, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Reader:\n",
    "        \n",
    "    def readData(self,filePath):\n",
    "        df= pd.read_csv(filePath)\n",
    "        return (df)\n",
    "reader = Reader()     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_of_Results= dict()\n",
    "dict_of_TrainingError= dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adultData_preprocessing(data,test_data):\n",
    "    \n",
    "    tS = np.array(test_data['Salary'])\n",
    "    tS = [x[:-1] for x in tS]\n",
    "    test_data['Salary'] = tS\n",
    "\n",
    "    le = LabelEncoder()\n",
    "    le.fit(data['Salary'])\n",
    "    data['Salary'] = le.transform(data['Salary'])\n",
    "    test_data['Salary'] = le.transform(test_data['Salary'])\n",
    "\n",
    "    le.fit(data['native-country'])\n",
    "    data['native-country'] = le.transform(data['native-country'])\n",
    "    test_data['native-country'] = le.transform(test_data['native-country'])\n",
    "\n",
    "    le.fit(data[\"marital-status\"])\n",
    "    data[\"marital-status\"] =le.transform(data[\"marital-status\"])\n",
    "    test_data['marital-status'] = le.transform(test_data['marital-status'])\n",
    "\n",
    "    le.fit(data[\"workclass\"])\n",
    "    data[\"workclass\"] =le.transform(data[\"workclass\"])\n",
    "    test_data['workclass'] = le.transform(test_data['workclass'])\n",
    "\n",
    "    le.fit(data[\"relationship\"])\n",
    "    data[\"relationship\"] =le.transform(data[\"relationship\"])\n",
    "    test_data['relationship'] = le.transform(test_data['relationship'])\n",
    "\n",
    "    le.fit(data[\"race\"])\n",
    "    data[\"race\"] =le.transform(data[\"race\"])\n",
    "    test_data['race'] = le.transform(test_data['race'])\n",
    "\n",
    "    le.fit(data[\"sex\"])\n",
    "    data[\"sex\"] =le.transform(data[\"sex\"])\n",
    "    test_data['sex'] = le.transform(test_data['sex'])\n",
    "\n",
    "    le.fit(data[\"occupation\"])\n",
    "    data[\"occupation\"] =le.transform(data[\"occupation\"])\n",
    "    test_data[\"occupation\"] =le.transform(test_data[\"occupation\"])\n",
    "\n",
    "    data = data.drop(['education'],axis=1)\n",
    "    test_data = test_data.drop(['education'],axis=1)\n",
    "    return data,test_data\n",
    "\n",
    "# Steel Plates\n",
    "def steelPlates_preprocessing(steelPlates_data):\n",
    "    columns = steelPlates_data.columns\n",
    "    label_names = ['Pastry', 'Z_Scratch', 'K_Scatch', 'Stains',\n",
    "       'Dirtiness', 'Bumps', 'Other_Faults']\n",
    "    steelPlates_data['label'] = steelPlates_data[label_names].apply(\n",
    "    lambda x: ''.join(x.dropna().astype(str)),\n",
    "    axis=1\n",
    "    )\n",
    "    steelPlates_data = steelPlates_data.drop(label_names,axis=1)\n",
    "    return steelPlates_data\n",
    "\n",
    "def yeast_preprocessing(yeast_data):\n",
    "    yeast_data= yeast_data.drop_duplicates()\n",
    "    yeast_data = yeast_data.drop(['Sequence Name'],axis=1)\n",
    "    return yeast_data\n",
    "def ThoraricSurgery_preprocessing(ThoraricSurgery_data):\n",
    "    cols = ThoraricSurgery_data.columns[:-1]\n",
    "    for col in cols:\n",
    "        if (str(ThoraricSurgery_data[col].dtype) == \"object\") and ((len(ThoraricSurgery_data[col].unique()))==2):\n",
    "            dummies = pd.get_dummies(ThoraricSurgery_data[col])\n",
    "            d_col =(dummies.columns)\n",
    "            dummies =dummies.rename(columns={d_col[0]: (col+ str(d_col[0])),\n",
    "                           d_col[1]: (col+str(d_col[1]))},errors=\"raise\")\n",
    "            ThoraricSurgery_data = pd.concat([ThoraricSurgery_data,dummies], axis=1)\n",
    "            ThoraricSurgery_data= ThoraricSurgery_data.drop(col,axis=1)\n",
    "            \n",
    "    le = LabelEncoder()\n",
    "    le.fit(ThoraricSurgery_data['Risk1Yr'])\n",
    "    ThoraricSurgery_data['Risk1Yr']=le.transform(ThoraricSurgery_data['Risk1Yr'])\n",
    "    \n",
    "    le.fit(ThoraricSurgery_data['DGN'])\n",
    "    ThoraricSurgery_data['DGN']=le.transform(ThoraricSurgery_data['DGN'])\n",
    "    \n",
    "    le.fit(ThoraricSurgery_data['PRE6'])\n",
    "    ThoraricSurgery_data['PRE6']=le.transform(ThoraricSurgery_data['PRE6'])\n",
    "    \n",
    "    le.fit(ThoraricSurgery_data['PRE14'])\n",
    "    ThoraricSurgery_data['PRE14'] =le.transform(ThoraricSurgery_data['PRE14'])\n",
    "    \n",
    "    \n",
    "    return ThoraricSurgery_data\n",
    "def seismic_preprocessing(seismic_data):\n",
    "    le = LabelEncoder()\n",
    "    for col in  seismic_data.columns:\n",
    "        if (str(seismic_data[col].dtype) == \"object\"):\n",
    "            le.fit(seismic_data[col])\n",
    "            seismic_data[col] = le.transform(seismic_data[col])\n",
    "            \n",
    "    return seismic_data\n",
    "\n",
    "def breastcancer_preprocessing(data):\n",
    "    data = data.drop(\"ID\", axis=1)\n",
    "    data['Diagnosis'].replace(0, 'M',inplace=True)\n",
    "    data['Diagnosis'].replace(1, 'B',inplace=True)\n",
    "    return data\n",
    "def creditcard_preprocessing(data):\n",
    "    new_header = data.iloc[0] #grab the first row for the header\n",
    "    data = data[1:] #take the data less the header row\n",
    "    data.columns = new_header #set the header row as the df header\n",
    "#     data = data.drop(\"ID\", axis=1)\n",
    "    return data\n",
    "\n",
    "def messidor_features_preprocessing(messidor_features_data):\n",
    "    le = LabelEncoder()\n",
    "    le.fit(messidor_features_data['Class'])\n",
    "    messidor_features_data['Class'] = le.transform(messidor_features_data['Class'])\n",
    "    return messidor_features_data\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(features, target):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42) \n",
    "\n",
    "    scaler_X = StandardScaler().fit(X_train)\n",
    "    scaler_y = StandardScaler().fit(y_train)\n",
    "\n",
    "    X_train_scaled = scaler_X.transform(X_train)\n",
    "    y_train_scaled = scaler_y.transform(y_train)\n",
    "\n",
    "    X_test_scaled = scaler_X.transform(X_test)\n",
    "    y_test_scaled = scaler_y.transform(y_test)\n",
    "    \n",
    "    return X_train_scaled, y_train_scaled, X_test_scaled, y_test_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling Large Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampling_data(X_train_scaled, y_train_scaled, X_test_scaled, y_test_scaled):\n",
    "    \n",
    "    id_train = np.random.randint(X_train_scaled.shape[0], size=5000)\n",
    "    X_train_scaled = X_train_scaled[id_train, :]\n",
    "    y_train_scaled = y_train_scaled[id_train, :]\n",
    "    \n",
    "    id_test = np.random.randint(X_test_scaled.shape[0], size=2000)\n",
    "    X_test_scaled = X_test_scaled[id_test, :]\n",
    "    y_test_scaled = y_test_scaled[id_test, :]\n",
    "    \n",
    "    return X_train_scaled, y_train_scaled, X_test_scaled, y_test_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table & Graph Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_table(dict_):\n",
    "    display(pd.DataFrame(dict_))\n",
    "\n",
    "def plot_graph(dict_,title):\n",
    "    data =list(dict_.keys())\n",
    "\n",
    "    x=np.array([0,1,2,3,4,5,6,7,8,9])\n",
    "    models = list(dict_.get(data[0]).keys())\n",
    "    results = pd.DataFrame(dict_).to_numpy()\n",
    "\n",
    "    plt.figure(figsize=(20,10))\n",
    "    for i in range(results.shape[0]):\n",
    "        plt.plot(results[i],label =models[i], marker='o')\n",
    "    \n",
    "    plt.xticks(x,data)\n",
    "    plt.xlabel(\"Datasets\")\n",
    "    plt.ylabel(\"Test Score\")\n",
    "    plt.title(title)\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_mse_cv(cv, X_train_scaled, y_train_scaled, X_test_scaled, y_test_scaled):\n",
    "    Rand_MSE_train = mean_squared_error (y_train_scaled, cv.best_estimator_.predict(X_train_scaled))\n",
    "    Rand_MSE_test = mean_squared_error (y_test_scaled,  cv.best_estimator_.predict(X_test_scaled))\n",
    "    return Rand_MSE_train, Rand_MSE_test\n",
    "    \n",
    "def prediction_mse(regressor, X_train_scaled, y_train_scaled, X_test_scaled, y_test_scaled):\n",
    "    Rand_MSE_train = mean_squared_error (y_train_scaled, regressor.predict(X_train_scaled))\n",
    "    Rand_MSE_test = mean_squared_error (y_test_scaled,  regressor.predict(X_test_scaled))\n",
    "    return Rand_MSE_train, Rand_MSE_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Model - Hyper Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_hyper_parameters(model, X_train_scaled, y_train_scaled):\n",
    "    if (X_train_scaled.flags['C_CONTIGUOUS'] == False):\n",
    "        X_train_scaled = X_train_scaled.copy(order='C')\n",
    "        \n",
    "    alphas = np.array([1,0.1,0.01,0.001,0.0001,0])\n",
    "    solver = ['auto' , 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag']\n",
    "    param_distributions = {\n",
    "         'alpha'     : alphas,\n",
    "         'solver'    : solver\n",
    "                        }\n",
    "    \n",
    "    gridcv = GridSearchCV(model, param_distributions, n_jobs=4)\n",
    "    gridcv.fit(X_train_scaled, y_train_scaled)\n",
    "    \n",
    "    return gridcv\n",
    "\n",
    "def svr_hyper_parameters(model, X_train_scaled, y_train_scaled):\n",
    "    sample_X_train = X_train_scaled\n",
    "    sample_y_train = y_train_scaled\n",
    "    row, column = sample_X_train.shape\n",
    "    if (row > 3000):\n",
    "        idx = np.random.randint(sample_X_train.shape[0], size=3000)\n",
    "        sample_X_train = sample_X_train[idx, :]\n",
    "        sample_y_train = sample_y_train[idx, :]\n",
    "    \n",
    "    param_distributions = {\n",
    "        'C'     : scipy.stats.reciprocal(1.0, 1000.),\n",
    "        'gamma' : scipy.stats.reciprocal(0.01, 10.),\n",
    "                          }\n",
    "    \n",
    "    randcv = RandomizedSearchCV(model, param_distributions,cv=2, n_iter=5 , verbose=1, random_state=0, n_jobs=4)\n",
    "    randcv.fit(sample_X_train, sample_y_train)\n",
    "        \n",
    "    return randcv\n",
    "\n",
    "def decission_tree_hyper_parameters(model, X_train_scaled, y_train_scaled):\n",
    "    param_distributions = {\n",
    "        'max_depth'     : scipy.stats.reciprocal(1, 1000)\n",
    "                          }\n",
    "    \n",
    "    randcv = RandomizedSearchCV(model, param_distributions, cv=2, n_iter=5 , verbose=1, random_state=0, n_jobs=8)\n",
    "    randcv.fit(X_train_scaled, y_train_scaled)\n",
    "    \n",
    "    return randcv\n",
    "\n",
    "def random_forest_hyper_parameters(model, X_train_scaled, y_train_scaled):\n",
    "    n_estimators_grid = [10, 100, 1000]\n",
    "    max_depth_grid = [10, 40, 70, 100]\n",
    "    param_grid = { 'n_estimators' : n_estimators_grid, 'max_depth' : max_depth_grid}\n",
    "    \n",
    "    randcv = GridSearchCV(model, param_grid, cv=5, verbose=1, n_jobs=4)\n",
    "    randcv.fit(X_train_scaled, y_train_scaled)\n",
    "    \n",
    "    return randcv\n",
    "\n",
    "def adaboost_hyper_parameters(model, X_train_scaled, y_train_scaled):\n",
    "    param_distributions = {\n",
    "         'n_estimators': [50, 100, 500],\n",
    "         'learning_rate' : [0.05,0.1,0.5,1],\n",
    "         'loss' : ['square', 'exponential']\n",
    "             }\n",
    "    \n",
    "    randcv = RandomizedSearchCV(model, param_distributions, cv=2, n_iter = 5, n_jobs=4, verbose=1, random_state=0)\n",
    "    randcv.fit(X_train_scaled, y_train_scaled)\n",
    "    \n",
    "    return randcv\n",
    "\n",
    "def mlp_hyper_parameters(model, X_train_scaled, y_train_scaled):\n",
    "    param_grid = {\n",
    "        'activation': ['logistic', 'tanh'],\n",
    "        'alpha': np.logspace(0.0001, 100, 10),\n",
    "        'hidden_layer_sizes': [\n",
    "            (30, 20), (40, 20), (90, 20)\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    gridcv = GridSearchCV(model, param_grid, cv=2, verbose=1, n_jobs=4)\n",
    "    gridcv.fit(X_train_scaled, y_train_scaled)\n",
    "    \n",
    "    return gridcv\n",
    "\n",
    "def gaussian_hyper_parameters(model, X_train_scaled, y_train_scaled):\n",
    "    sample_X_train = X_train_scaled\n",
    "    sample_y_train = y_train_scaled\n",
    "    row, column = sample_X_train.shape\n",
    "    if (row > 3000):\n",
    "        idx = np.random.randint(sample_X_train.shape[0], size=3000)\n",
    "        sample_X_train = sample_X_train[idx, :]\n",
    "        sample_y_train = sample_y_train[idx, :]\n",
    "    \n",
    "    kernels = [\n",
    "        RBF(1.0),\n",
    "        RBF(2.0),\n",
    "        RationalQuadratic(5.0, 0.02)\n",
    "    ]\n",
    "    param_distributions = {\n",
    "         'kernel'     : kernels\n",
    "                      }\n",
    "    \n",
    "    gridcv = GridSearchCV(model, param_distributions,cv=5, verbose=1, n_jobs=-1)\n",
    "    gridcv.fit(sample_X_train, sample_y_train)\n",
    "    \n",
    "    return gridcv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classificaiton Models - Hyper Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Models(X_train,y_train,X_test,y_test):\n",
    "    \n",
    "    Results ={}\n",
    "    TrainingError ={}\n",
    "#Logistic\n",
    "    print(\"Logistic regression - ClassifIER\")\n",
    "    penalty = ['l1', 'l2']\n",
    "    C = np.logspace(0, 4, 10)\n",
    "    param_grid=dict(C=C,penalty=penalty)\n",
    "    clf = LogisticRegression(random_state=0)\n",
    "    gridcv= GridSearchCV(clf,param_grid,cv=5,verbose=1)\n",
    "    gridcv.fit(X_train, y_train)\n",
    "    print(\"Using best parameters found for Logistic regression\")\n",
    "    Results.update({\"LR\":gridcv.best_estimator_.score(X_test,y_test)})\n",
    "    print(\"Saving result.....\")\n",
    "    TrainingError.update({\"LR\":gridcv.best_estimator_.score(X_train,y_train)})\n",
    "    \n",
    "#K- nearest Neighbors\n",
    "    print(\"K- nearest Neighbors Classifier\")\n",
    "    neigh = KNeighborsClassifier()        \n",
    "    n_neighbors_grid=np.array([5,7,9,11,13,17])\n",
    "    param_grid = { 'n_neighbors' : n_neighbors_grid}\n",
    "    gridcv = GridSearchCV(neigh, param_grid, verbose=1, cv=3)    \n",
    "    gridcv.fit(X_train,y_train) \n",
    "    print(\"Using best parameters found for K- nearest Neighbors\")\n",
    "    Results.update({\"KNN\":gridcv.best_estimator_.score(X_test,y_test)})\n",
    "    print(\"Saving result.....\")\n",
    "    TrainingError.update({\"KNN\":gridcv.best_estimator_.score(X_train,y_train)})\n",
    "\n",
    "#SVM\n",
    "    print(\"Support Vector Machine Classifier\")\n",
    "    svm = SVC(kernel='rbf').fit(X_train,y_train)\n",
    "    Results.update({\"SVM\":svm.score(X_test,y_test)})\n",
    "    print(\"Saving result.....\")\n",
    "    TrainingError.update({\"SVM\":svm.score(X_train,y_train)})\n",
    "    \n",
    "#Random Forest\n",
    "    print(\"Random Forest Classifier\")\n",
    "    n_estimators=np.array([20,30])\n",
    "    max_depth=np.array([5,7,10])\n",
    "    rfc = RandomForestClassifier(random_state=0)\n",
    "    param_grid ={ 'n_estimators' : n_estimators, 'max_depth' : max_depth}\n",
    "    print(\"Using best parameters found for Random Forest\")\n",
    "    gridcv = GridSearchCV(rfc, param_grid, verbose=1, cv=3).fit(X_train,y_train)\n",
    "        \n",
    "    print(\"Saving result.....\")\n",
    "    Results.update({\"RandomForest\":gridcv.best_estimator_.score(X_test,y_test)})\n",
    "    TrainingError.update({\"RandomForest\":gridcv.best_estimator_.score(X_train,y_train)})\n",
    "    \n",
    "#AdaBooster\n",
    "    print(\"Ada Booster Classifier\")\n",
    "    n_estimators=np.array([30,40])\n",
    "    ada = AdaBoostClassifier(random_state=0)\n",
    "    param_grid ={ 'n_estimators' : n_estimators}\n",
    "    gridcv = GridSearchCV(ada, param_grid, verbose=1, cv=3).fit(X_train,y_train)\n",
    "    print(\"Running for best parameter search for AdaBoost\")\n",
    "    print(\"Saving results....\")\n",
    "    Results.update({\"AdaBooster\":gridcv.best_estimator_.score(X_test,y_test)})\n",
    "    TrainingError.update({\"AdaBooster\":gridcv.best_estimator_.score(X_train,y_train)})\n",
    "    \n",
    "#Gaussian Classifier\n",
    "    print(\"Gaussian Classifier\")\n",
    "    gnb = GaussianNB()\n",
    "    y_pred = gnb.fit(X_train, y_train).predict(X_test)\n",
    "    actual_y = np.array(y_test)\n",
    "    print(\"Running for best parameter search for gaussian Classifier\")\n",
    "    Results.update({\"GNB\":gnb.score(X_test,y_test)})\n",
    "    print(\"Saving results....\")\n",
    "    TrainingError.update({\"GNB\":gnb.score(X_train,y_train)})\n",
    "#Tree\n",
    "\n",
    "    print(\"Decision Tree Classifier\")\n",
    "    cret=np.array(['gini','entropy'])\n",
    "    max_depth=np.array([10,20])\n",
    "    param_grid ={ 'criterion' : cret, 'max_depth' : max_depth}\n",
    "    tree = DecisionTreeClassifier(random_state=0)\n",
    "    print(\"Running for best parameter search for Decision Tree\")\n",
    "    gridcv = GridSearchCV(tree, param_grid, verbose=1, cv=3).fit(X_train,y_train)\n",
    "    print(\"Saving result.....\")\n",
    "    Results.update({\"DecisionTree\":gridcv.score(X_test,y_test)})\n",
    "    TrainingError.update({\"Tree\":gnb.score(X_train,y_train)})\n",
    "\n",
    "#MLP\n",
    "    print(\"Multilayer perceptron\")\n",
    "    mlp = MLPClassifier()\n",
    "    mlp.fit(X_train,y_train)\n",
    "    print(\"Saving result.....\")\n",
    "    Results.update({\"MLP\":mlp.score(X_test,y_test)})\n",
    "    TrainingError.update({\"MLP\":mlp.score(X_train,y_train)})\n",
    "    \n",
    "    return Results,TrainingError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regression(X_train_scaled, y_train_scaled, X_test_scaled, y_test_scaled):\n",
    "    model_dict_test={}\n",
    "    model_dict_train={}\n",
    "    models = [\n",
    "        LinearRegression (fit_intercept=False, normalize=False, copy_X=True, n_jobs=None),\n",
    "        Ridge(fit_intercept=True, normalize=False, copy_X=True),\n",
    "        SVR(kernel='rbf'),\n",
    "        DecisionTreeRegressor(random_state=0),\n",
    "        RandomForestRegressor(random_state=0),\n",
    "        AdaBoostRegressor(base_estimator = None, random_state=0),\n",
    "        MLPRegressor(learning_rate='adaptive', max_iter=500, random_state=0),\n",
    "        GaussianProcessRegressor(normalize_y=False, random_state=0)\n",
    "    ]\n",
    "    \n",
    "    for model in models: \n",
    "        if (type(model).__name__ == \"LinearRegression\"):\n",
    "            model.fit(X_train_scaled, y_train_scaled)\n",
    "            train_mse, test_mse = prediction_mse(model, X_train_scaled, y_train_scaled, X_test_scaled, y_test_scaled)\n",
    "            print('    Linear Regression - Done')\n",
    "        \n",
    "        if (type(model).__name__ == \"Ridge\"):\n",
    "            cv = ridge_hyper_parameters(model, X_train_scaled, y_train_scaled)\n",
    "            train_mse, test_mse = prediction_mse_cv(cv, X_train_scaled, y_train_scaled, X_test_scaled, y_test_scaled)\n",
    "            print('    Ridge Regression - Done')\n",
    "        \n",
    "        if (type(model).__name__ == \"SVR\"):\n",
    "            cv = svr_hyper_parameters(model, X_train_scaled, y_train_scaled)\n",
    "            train_mse, test_mse = prediction_mse_cv(cv, X_train_scaled, y_train_scaled, X_test_scaled, y_test_scaled)\n",
    "            print('    Support Vector Regression - Done')\n",
    "        \n",
    "        if (type(model).__name__ == \"DecisionTreeRegressor\"):\n",
    "            cv = decission_tree_hyper_parameters(model, X_train_scaled, y_train_scaled)\n",
    "            train_mse, test_mse = prediction_mse_cv(cv, X_train_scaled, y_train_scaled, X_test_scaled, y_test_scaled)\n",
    "            print('    Decission Tree Regression - Done')\n",
    "        \n",
    "        if (type(model).__name__ == \"RandomForestRegressor\"):\n",
    "            cv = random_forest_hyper_parameters(model, X_train_scaled, y_train_scaled)\n",
    "            train_mse, test_mse = prediction_mse_cv(cv, X_train_scaled, y_train_scaled, X_test_scaled, y_test_scaled)\n",
    "            print('    Random Forest Regression - Done')\n",
    "        \n",
    "        if (type(model).__name__ == \"AdaBoostRegressor\"):\n",
    "            cv = adaboost_hyper_parameters(model, X_train_scaled, y_train_scaled)\n",
    "            train_mse, test_mse = prediction_mse_cv(cv, X_train_scaled, y_train_scaled, X_test_scaled, y_test_scaled)\n",
    "            print('    AdaBoost Regression - Done')\n",
    "        \n",
    "        if (type(model).__name__ == \"MLPRegressor\"):\n",
    "            cv = mlp_hyper_parameters(model, X_train_scaled, y_train_scaled)\n",
    "            train_mse, test_mse = prediction_mse_cv(cv, X_train_scaled, y_train_scaled, X_test_scaled, y_test_scaled)\n",
    "            print('    Neural Network Regression - Done')\n",
    "        \n",
    "        if (type(model).__name__ == \"GaussianProcessRegressor\"):\n",
    "            cv = gaussian_hyper_parameters(model, X_train_scaled, y_train_scaled)\n",
    "            train_mse, test_mse = prediction_mse_cv(cv, X_train_scaled, y_train_scaled, X_test_scaled, y_test_scaled)\n",
    "            print('    Gaussian Process Regression - Done')\n",
    "        \n",
    "        model_dict_test.update({type(model).__name__:test_mse})\n",
    "        model_dict_train.update({type(model).__name__:train_mse})\n",
    "    \n",
    "    return model_dict_train, model_dict_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Loading & Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wine_data_set():\n",
    "    features, target = load_wine_data_set()\n",
    "    X_train_scaled, y_train_scaled, X_test_scaled, y_test_scaled = split_data(features, target)\n",
    "    print('Wine:')\n",
    "    return regression(X_train_scaled, y_train_scaled, X_test_scaled, y_test_scaled)\n",
    "\n",
    "def communities_data_set():\n",
    "    features, target = load_communities_data_set()\n",
    "    X_train_scaled, y_train_scaled, X_test_scaled, y_test_scaled = split_data(features, target)\n",
    "    print('Communities:')\n",
    "    return regression(X_train_scaled, y_train_scaled, X_test_scaled, y_test_scaled)\n",
    "    \n",
    "def qasr_data_set():\n",
    "    features, target = load_qasr_data_set()\n",
    "    X_train_scaled, y_train_scaled, X_test_scaled, y_test_scaled = split_data(features, target)\n",
    "    print('QASR:')\n",
    "    return regression(X_train_scaled, y_train_scaled, X_test_scaled, y_test_scaled)\n",
    "    \n",
    "def parkinson_data_set():\n",
    "    features, target = load_parkinson_data_set()\n",
    "    X_train_scaled, y_train_scaled, X_test_scaled, y_test_scaled = split_data(features, target)\n",
    "    print('Parkinson:')\n",
    "    return regression(X_train_scaled, y_train_scaled, X_test_scaled, y_test_scaled)\n",
    "    \n",
    "def facebook_data_set():\n",
    "    features, target = load_facebook_data_set()\n",
    "    X_train_scaled, y_train_scaled, X_test_scaled, y_test_scaled = split_data(features, target)\n",
    "    print('Facebook:')\n",
    "    return regression(X_train_scaled, y_train_scaled, X_test_scaled, y_test_scaled)\n",
    "    \n",
    "def bike_data_set():\n",
    "    features, target = load_bike_data_set()\n",
    "    X_train_scaled, y_train_scaled, X_test_scaled, y_test_scaled = split_data(features, target)\n",
    "    print('Bike:')\n",
    "    return regression(X_train_scaled, y_train_scaled, X_test_scaled, y_test_scaled)\n",
    "\n",
    "def concrete_data_set():\n",
    "    features, target = load_concrete_data_set()\n",
    "    X_train_scaled, y_train_scaled, X_test_scaled, y_test_scaled = split_data(features, target)\n",
    "    print('Concrete:')\n",
    "    return regression(X_train_scaled, y_train_scaled, X_test_scaled, y_test_scaled)\n",
    "\n",
    "def student_data_set():\n",
    "    features, target = load_student_data_set()\n",
    "    X_train_scaled, y_train_scaled, X_test_scaled, y_test_scaled = split_data(features, target)\n",
    "    print('Student:')\n",
    "    return regression(X_train_scaled, y_train_scaled, X_test_scaled, y_test_scaled)\n",
    "    \n",
    "def sgemm_data_set():\n",
    "    features, target = load_sgemm_data_set()\n",
    "    X_train_scaled, y_train_scaled, X_test_scaled, y_test_scaled = split_data(features, target)\n",
    "    X_train_scaled, y_train_scaled, X_test_scaled, y_test_scaled = sampling_data(X_train_scaled, y_train_scaled, X_test_scaled, y_test_scaled)\n",
    "    print('SGEMM:')\n",
    "    return regression(X_train_scaled, y_train_scaled, X_test_scaled, y_test_scaled)\n",
    "\n",
    "def merck_data_set():\n",
    "    features, target = load_merck_data_set()\n",
    "    X_train_scaled, y_train_scaled, X_test_scaled, y_test_scaled = split_data(features, target)\n",
    "    print('Merck:')\n",
    "    return regression(X_train_scaled, y_train_scaled, X_test_scaled, y_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Adult_data= reader.readData(r\"adult\\adult_data.csv\")\n",
    "Adult_Test = reader.readData(r\"adult\\adult_test_set.csv\")\n",
    "\n",
    "steelPlates_data= reader.readData(r\"SteelPlates\\SteelPlates.csv\")\n",
    "yeast_data= reader.readData(r\"yeast\\yeast.csv\")\n",
    "ThoraricSurgery_data = arff.loadarff(r'thropatic\\ThoraricSurgery.arff')\n",
    "ThoraricSurgery_data = pd.DataFrame(ThoraricSurgery_data[0])\n",
    "\n",
    "seismic_data= reader.readData(r\"sesmic\\seismic-bumps.csv\")\n",
    "\n",
    "messidor_features_data=arff.loadarff(r'Diabetic\\messidor_features.arff')\n",
    "messidor_features_data = pd.DataFrame(messidor_features_data[0])\n",
    "\n",
    "creditcard_data=reader.readData(r'CreditCard\\creditcard_default.csv')\n",
    "breastcancer_data=reader.readData(r'breastCancer\\breastcancer.csv')\n",
    "ausCrediApproval_data=reader.readData(r'StatlogAus\\ausCreditApproval.csv')\n",
    "GermanCredit_data=reader.readData(r'StatlogGerman\\GermanCredit.csv')\n",
    "\n",
    "\n",
    "\n",
    "Adult_data, Adult_Test = adultData_preprocessing(Adult_data, Adult_Test)\n",
    "steelPlates_data = steelPlates_preprocessing(steelPlates_data)\n",
    "yeast_data= yeast_preprocessing(yeast_data)\n",
    "\n",
    "\n",
    "ThoraricSurgery_data = ThoraricSurgery_preprocessing(ThoraricSurgery_data)\n",
    "\n",
    "seismic_data=seismic_data.drop_duplicates()\n",
    "seismic_data = seismic_preprocessing(seismic_data)\n",
    "\n",
    "creditcard_data= creditcard_preprocessing(creditcard_data)\n",
    "\n",
    "breastcancer_data= breastcancer_preprocessing(breastcancer_data)\n",
    "\n",
    "messidor_features_data = messidor_features_preprocessing(messidor_features_data)\n",
    "\n",
    "DataSets = { \"steelPlates_data\":steelPlates_data,\n",
    "             \"yeast_data\":yeast_data,\n",
    "            \"ThoraricSurgery_data\":ThoraricSurgery_data,\n",
    "            \"seismic_data\":seismic_data,\n",
    "           \"messidor_features_data\":messidor_features_data,\n",
    "           \"creditcard_data\":creditcard_data,\n",
    "           \"breastcancer_data\":breastcancer_data,\n",
    "           \"ausCrediApproval_data\":ausCrediApproval_data,\n",
    "           \"GermanCredit_data\":GermanCredit_data}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dict_train={}\n",
    "result_dict_test={}\n",
    "def predict():\n",
    "    train, test = wine_data_set()\n",
    "    result_dict_train.update({'Wine':train})\n",
    "    result_dict_test.update({'Wine':test})\n",
    "    \n",
    "    train, test = communities_data_set()\n",
    "    result_dict_train.update({'Communities':train})\n",
    "    result_dict_test.update({'Communities':test})\n",
    "    \n",
    "    train, test = qasr_data_set()\n",
    "    result_dict_train.update({'QASR':train})\n",
    "    result_dict_test.update({'QASR':test})\n",
    "    \n",
    "    train, test = parkinson_data_set()\n",
    "    result_dict_train.update({'Parkinson':train})\n",
    "    result_dict_test.update({'Parkinson':test})\n",
    "    \n",
    "    train, test = facebook_data_set()\n",
    "    result_dict_train.update({'Facebook':train})\n",
    "    result_dict_test.update({'Facebook':test})\n",
    "    \n",
    "    train, test = bike_data_set()\n",
    "    result_dict_train.update({'Bike':train})\n",
    "    result_dict_test.update({'Bike':test})\n",
    "    \n",
    "    train, test = concrete_data_set()\n",
    "    result_dict_train.update({'Concrete':train})\n",
    "    result_dict_test.update({'Concrete':test})\n",
    "    \n",
    "    train, test = student_data_set()\n",
    "    result_dict_train.update({'Student':train})\n",
    "    result_dict_test.update({'Student':test})\n",
    "    \n",
    "    train, test = sgemm_data_set()\n",
    "    result_dict_train.update({'SGEMM':train})\n",
    "    result_dict_test.update({'SGEMM':test})\n",
    "    \n",
    "    train, test = merck_data_set()\n",
    "    result_dict_train.update({'Merck':train})\n",
    "    result_dict_test.update({'Merck':test})\n",
    "    \n",
    "    plot_table(result_dict_train)\n",
    "    plot_table(result_dict_test)\n",
    "    \n",
    "    plot_graph(result_dict_train, 'Train MSE')\n",
    "    plot_graph(result_dict_test, 'Test MSE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createTestTrain(data,name):\n",
    "    scaler = StandardScaler()\n",
    "    if name == \"steelPlates_data\":\n",
    "        X = np.array(data.drop(['label'],axis=1))\n",
    "        y = np.array(data['label'])\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "        return X_train, X_test, y_train, y_test\n",
    "    if name == \"yeast_data\":\n",
    "        X = np.array(data.drop(['Label'],axis=1))\n",
    "        y = np.array(data['Label'])\n",
    "\n",
    "      \n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "          \n",
    "        scaler.fit(X_train)\n",
    "        X_train = scaler.transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "        return X_train, X_test, y_train, y_test\n",
    "    if name == \"ThoraricSurgery_data\":\n",
    "        X = np.array(data.drop(['Risk1Yr'],axis=1))\n",
    "        y = np.array(data['Risk1Yr'])\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "        return X_train, X_test, y_train, y_test\n",
    "    if name ==\"seismic_data\":\n",
    "        X = np.array(data.drop(['class'],axis=1))\n",
    "        y = np.array(data['class'])\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "        return X_train, X_test, y_train, y_test\n",
    "    if name ==\"messidor_features_data\":\n",
    "        X = np.array(data.drop(['Class'],axis=1))\n",
    "        y = np.array(data['Class'])\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "        return  X_train, X_test, y_train, y_test\n",
    "    if name ==\"creditcard_data\":\n",
    "        X = np.array(data.drop(['default payment next month'],axis=1))\n",
    "        y = np.array(data['default payment next month'])\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "        scaler.fit(X_train)\n",
    "        X_train = scaler.transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "\n",
    "        return  X_train, X_test, y_train, y_test \n",
    "    if name ==\"breastcancer_data\":\n",
    "        X = np.array(data.drop(['Diagnosis'],axis=1))\n",
    "        y = np.array(data['Diagnosis'])\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) \n",
    "        return  X_train, X_test, y_train, y_test \n",
    "    if name ==\"GermanCredit_data\":\n",
    "        X = np.array(data.drop(['Column26'],axis=1))\n",
    "        y = np.array(data['Column26'])\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "        return  X_train, X_test, y_train, y_test \n",
    "    if name ==\"ausCrediApproval_data\":\n",
    "        X = np.array(data.drop(['A15'],axis=1))\n",
    "        y = np.array(data['A15'])\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "        return  X_train, X_test, y_train, y_test \n",
    "    else:\n",
    "        y_train = Adult_data['Salary']\n",
    "        y_test = Adult_Test['Salary']\n",
    "        X_train = Adult_data.drop(['Salary'],axis=1)\n",
    "        X_test = Adult_Test.drop(['Salary'],axis=1)\n",
    "        return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getResults(DataSets,dict_of_Results):\n",
    "\n",
    "    for data in DataSets.keys():\n",
    "        print(\"Results for\",data,end='')\n",
    "        print('')\n",
    "        X_train, X_test, y_train, y_test =createTestTrain(DataSets.get(data),data)\n",
    "        test,train = Models(X_train,y_train,X_test,y_test)\n",
    "        dict_of_Results.update({data:test})\n",
    "        dict_of_TrainingError.update({data:train})\n",
    "    y_train = Adult_data['Salary']\n",
    "    y_test = Adult_Test['Salary']\n",
    "    X_train = Adult_data.drop(['Salary'],axis=1)\n",
    "    X_test = Adult_Test.drop(['Salary'],axis=1)\n",
    "    test,train = Models(X_train,y_train,X_test,y_test)\n",
    "    dict_of_Results.update({\"AdultDatabase\":test})\n",
    "    dict_of_TrainingError.update({data:train})\n",
    "    return dict_of_Results, dict_of_TrainingError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classification Call\n",
    "dict_of_Results, dict_of_TrainingError=getResults(DataSets,dict_of_Results)\n",
    "\n",
    "plot_table(dict_of_Results)\n",
    "plot_table(dict_of_TrainingError)\n",
    "    \n",
    "plot_graph(dict_of_Results, 'Test Accuracy')\n",
    "plot_graph(dict_of_TrainingError, 'Train Accuracy')\n",
    "\n",
    "# Regression Call\n",
    "predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
